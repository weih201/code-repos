print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,sqrt(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
#Train the neural network
#Going to have 10 hidden layers
#Threshold is a numeric value specifying the threshold for the partial
#derivatives of the error function as stopping criteria.
net.sqrt <- neuralnet(Output~Input,trainingdata, hidden=c(10,10), threshold=0.01)
print(net.sqrt)
#Plot the neural network
plot(net.sqrt)
#Test the neural network on some training data
testdata <- as.data.frame((1:10)^2) #Generate some squared numbers
net.results <- compute(net.sqrt, testdata) #Run them through the neural network
#Lets see what properties net.sqrt has
ls(net.results)
#Lets see the results
print(net.results$net.result)
#Lets display a better version of the results
cleanoutput <- cbind(testdata,sqrt(testdata),
as.data.frame(net.results$net.result))
colnames(cleanoutput) <- c("Input","Expected Output","Neural Net Output")
print(cleanoutput)
summary(lm(Fertility ~ . , data = swiss))
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
n <- 100; x2 <- 1 : n; x1 <- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
summary(lm(y ~ x1))$coef
lines(xVals, yVals - 2 * se1)
summary(lm(y ~ x1 + x2))$coef
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
p1 <- predict(fit, newdata, interval = ("confidence"))
summary(lm(count ~ spray, data = InsectSprays))$coef
summary(lm(count ~
I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
I(1 * (spray == 'F'))
, data = InsectSprays))$coef
lm(count ~
I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
I(1 * (spray == 'F')) + I(1 * (spray == 'A')), data = InsectSprays)
summary(lm(count ~ spray - 1, data = InsectSprays))$coef
unique(ave(InsectSprays$count, InsectSprays$spray))
fit <- lm(count ~ spray, data = InsectSprays) #A is ref
bbmbc <- coef(fit)[2] - coef(fit)[3] #B - C
temp <- summary(fit)
se <- temp$sigma * sqrt(temp$cov.unscaled[2, 2] + temp$cov.unscaled[3,3] - 2 *temp$cov.unscaled[2,3])
t <- (bbmbc) / se
p <- pt(-abs(t), df = fit$df)
out <- c(bbmbc, se, t, p)
names(out) <- c("B - C", "SE", "T", "P")
round(out, 3)
#download.file("http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*","hunger <- read.csv("hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
head(hunger)
#######
download.file("http://apps.who.int/gho/athena/data/GHO/WHOSIS_000008.csv?profile=text&filter=COUNTRY:*;SEX:*","hunger <- read.csv("hunger.csv")
hunger <- hunger[hunger$Sex!="Both sexes",]
head(hunger)
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
mtcars
mtcars
fit<-lm(mpg ~ cyl+ wt, data=mtcars)
summary(fit)
summary(fit)$coefficients
fit<-lm(mpg ~ cyl+ wt, data=mtcars)
coef<-summary(fit)$coefficients
coef$Estimate
length(coef)
coef
dim(coef)
dim(coef)
coef[Estimate,]
summary(coef)
coef
coef[2,1]
coef[2,1]*(8-4)
fit<-lm(mpg ~ cyl+ wt, data=mtcars)
coef<-summary(fit)$coefficients
coef
fit<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef<-summary(fit)$coefficients
dim(coef)
coef
summary(mtcars$cyl)
factor(mtcars$cyl)
fit1<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef_adj<-summary(fit1)$coefficients
fit2<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef_unadj<-summary(fit2)$coefficients
coef_adj
coef_unadj
fit2<-lm(mpg ~ factor(cyl), data=mtcars)
coef_unadj<-summary(fit2)$coefficients
coef_adj
coef_unadj
fit1<-lm(mpg ~ factor(cyl)+ wt + factor(cyl)*wt, data=mtcars)
coef1<-summary(fit1)$coefficients
fit2<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef2<-summary(fit2)$coefficients
coef1
coef2
fit1<-lm(mpg ~ factor(cyl)+ wt + factor(cyl)*wt, data=mtcars)
coef1<-summary(fit1)$coefficients
fit2<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef2<-summary(fit2)$coefficients
coef1
coef2
summary(fit1)
summary(fit2)
# q3
fit1<-lm(mpg ~ factor(cyl)+ wt + cyl*wt, data=mtcars)
coef1<-summary(fit1)$coefficients
fit2<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef2<-summary(fit2)$coefficients
coef1
coef2
summary(fit1)
summary(fit2)
?lrtest
lrtest
install.packages("lmtest")
library("lmtest")
?interaction
fit1<-lm(mpg ~ factor(cyl)+ wt + interaction(cyl,wt), data=mtcars)
coef1<-summary(fit1)$coefficients
fit2<-lm(mpg ~ factor(cyl)+ wt, data=mtcars)
coef2<-summary(fit2)$coefficients
coef1
coef2
summary(fit1)
summary(fit2)
?vif
library(car)
?vif
library("lmtest")
library("car")
vif(fit1)
vif(fit2)
anova(fit1,fit2)
library("lmtest")
lrtest(fit1,fit2)
anova(fit2,fit1)
anova(fit1)
anova(fit2)
fit<-lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
fit
?hatvalue
?hatvalues
fit <- lm(y ~ x)
round(dfbetas(fit)[1 : 10, 2], 3)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
hatvalues(fit)
?influence.measure
?influence.measures
influence.measures(fit)
?round
?dfbetas
hatvalues(fit)
dfbeta(fit)
dfbetas(fit)
plot(x,y)
h <- hat(model.matrix(fit))
h
model.matrix(fit)
?hat
?(model.matrix
)
?model.matrix
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
plot(x,y)
fit <- lm(y ~ x)
hat(model.matrix(fit))
dfbetas(fit)
dfbetas(fit)
dfbeta(fit)
?dfbetas
dfbetas(fit)
mtcars
fit<-lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
fit
summary(mtcars)
?mtcars
install.packages("pyramid")
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
devtools::install_github("babynames", "hadley")
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages(c("devtools", "ggplot2", "knitr", "yaml", "htmltools"))
install.packages("mapproj")
install.packages("ggmap")
install.packages("DeducerSpatial")
require(maps)
## Loading required package: maps
require(ggmap)
## Loading required package: ggmap
## Loading required package: ggplot2
par(mfrow = c(2, 1))
map("usa")
map("county")
map("world", "China")
map.cities(country = "China", capitals = 2)
map("state", "GEORGIA")
data(us.cities)
map.cities(us.cities, country = "GA")
data(unemp)
data(county.fips)
# Plot unemployment by country
colors = c("#F1EEF6", "#D4B9DA", "#C994C7", "#DF65B0", "#DD1C77",
"#980043")
head(unemp)
unemp$colorBuckets <- as.numeric(cut(unemp$unemp, c(0, 2, 4, 6, 8,
10, 100)))
colorsmatched <- unemp$colorBuckets[match(county.fips$fips, unemp$fips)]
map("county", col = colors[colorsmatched], fill = TRUE, resolution = 0,
lty = 0, projection = "polyconic")
# Add border around each State
map("state", col = "white", fill = FALSE, add = TRUE, lty = 1, lwd = 0.2,
projection = "polyconic")
title("unemployment by county, 2009")
leg.txt <- c("<2%", "2-4%", "4-6%", "6-8%", "8-10%", ">10%")
legend("topright", leg.txt, horiz = TRUE, fill = colors)
## Deducer
library(UScensus2000)
lat <- c(43.834526782236814,30.334953881988564)
lon <- c(-131.0888671875  ,-107.8857421875)
southwest <- openmap(c(lat[1],lon[1]),c(lat[2],lon[2]),5,'bing')
data(california.tract)
california.tract <- spTransform(california.tract,osm())
plot(southwest,removeMargin=TRUE)
choro_plot(california.tract,dem = slot(california.tract,"data")[,'med.age'], legend.title = 'Median Age',alpha=1)
install.packages("UScensus2000")
## Using the ggmap package
geocode('CDC')
geocode('CDC')
geocode('Baylor University')
geocode('the white house', messaging = TRUE)
geocode(c('baylor university', 'CDC'), output = 'latlona')
geocode(c('harvard university', 'the vatican'), output = 'more')
geocode('the eiffel tower', output = 'all')
mapdist(from, to,
mode = c("driving", "walking", "bicycling"),
output = c("simple", "all"), messaging = FALSE,
sensor = FALSE, language = "en-EN",
override_limit = FALSE)
mapdist('CDC', 'the white house', mode = 'walking')
## Study of crimes in Houston
violent_crimes <- subset(crime, offense != "auto theft" & offense !=
"theft" & offense != "burglary")
# rank violent crimes
violent_crimes$offense <- factor(violent_crimes$offense, levels = c("robbery",
"aggravated assault", "rape", "murder"))
# restrict to downtown
violent_crimes <- subset(violent_crimes, -95.39681 <= lon & lon <=
-95.34188 & 29.73631 <= lat & lat <= 29.784)
## Map these crimes on the map of the city
HoustonMap <- qmap('houston', zoom = 14,color = 'bw', legend = 'topleft')
HoustonMap +geom_point(aes(x = lon, y = lat,
size = offense,colour = offense), data = violent_crimes )
## Plot again but use stats$_$denisty layer
houston <- get_map('houston', zoom = 14)
HoustonMap <- ggmap(houston, extent = 'device', legend = 'topleft')
HoustonMap + stat_density2d(aes(x = lon, y = lat,
fill = ..level.. , alpha = ..level..),size = 2, bins = 4,
data = violent_crimes, geom = 'polygon')
scale_fill_gradient('Violent\nCrime\nDensity') +
scale_alpha(range = c(.4, .75), guide = FALSE) +
guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
## Add the colorbar guide to the key
HoustonMap
+  stat_density2d(aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),
size = 2, bins = 4, data = violent_crimes, geom = 'polygon')
+scale_fill_gradient('Violent\nCrime\nDensity')
+scale_alpha(range = c(.4, .75), guide = FALSE)
+guides(fill = guide_colorbar(barwidth = 1.5, barheight = 10))
### Results of qmap using ggmap of crimes in houston
sessionInfo()
mtcars
swirl()
library(swirl)
ls()
rm(ls())
rm(list=ls())
ls()
sqirl()
swirl()
lm(slider)
head(slider)
type_info()
type info()
l_nor<-lm(gch_nor~gpa_nor)
fit<-lm(child ~ parent,data=galton)
fit<-lm(child ~ parent,galton)
n
sqrt(fit$residuals^2)/(n-2)
sqrt(sum(fit$residuals^2))/(n-2)
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
deviance(fit)/(n-2)
sqrt(deviance(fit)/(n-2))
mu<-mean(galton$child)
sTot<-sum((galton$child-mu)^2)
sRes<-deviance(glaton$child)
sRes<-deviance(galton$child)
sRes<-sum((galton$child-fit)^2)
fit
sRes<-deviance(fit)
1-sRes/sTot
summary(fit)$r
summary(fit)$r.squared
cor(galton$child,galton$parent)
cor(galton$child,galton$parent)^2
ones<-rep(1,nrow(galton))
lm(child~ones+parent-1,glaton)
lm(child~ones+parent-1,galton)
lm(child~parent,galton)
lm(child~1,galton)
lm(child~2,galton)
head(tress)
head(trees)
fit<-lm(Volume ~ Girth + Height + Constant -1, trees)
trees2<-eliminate("Girth",trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant
| -1, trees2)
fit2 <- lm(Volume ~ . - 1, trees2)
lapply(list(fit, fit2), coef)
lm(child~parent,galton)
lm(child~1,galton)
n<-nrow(galton)
n
noes<-rep(1,n)
noes
lm(child~noes,galton)
lm(child~parent,galton)
lm(child~parent+noes,galton)
lm(child~noes,galton)
lm(child~noes+parent,galton)
lm(child~noes+parent -1,galton)
lm(child~parent -1,galton)
lm(child~parent,galton)
?lm
lm(Volume ~ . - 1, trees2)
lm(Volume ~ . , trees2)
install.packages("MiKTeX")
## File name: datacleanPrj.R
## This file is the surce code for the Getting and Cleanning Data course project, which
## is to address following tasks:
## 1.Merges the training and the test sets to create one data set.
## 2.Extracts only the measurements on the mean and standard deviation for each measurement.
## 3.Uses descriptive activity names to name the activities in the data set
## 4.Appropriately labels the data set with descriptive variable names.
## 5.Creates a second, independent tidy data set with the average of each variable for each activity and each subject.
## Setting work directory
setwd("C:/my home/coursera/Data Science Specialization/getting and clean data/course project/UCI HAR Dataset")
list.files()
## Inout train and test data
dfTrain_X<-read.table("./train/X_train.txt")
dfTest_X<-read.table("./test/X_test.txt")
df_X<-rbind(dfTrain_X,dfTest_X)
## Read features and assign to data frame
features<-read.table("features.txt",stringsAsFactors=FALSE)
colnames(df_X)<-features$V2
## Select the mean and standard deviztion cols and subsetting original data set
keep<-grepl("mean\\(\\)|std\\(\\)", names(df_X), ignore.case = TRUE)
dfx_Keep<-df_X[, keep]
## Read the activity labels
labels<-read.table("activity_labels.txt")
## Read the subject and training data set for each activity
sub_train<-read.table("./train/subject_train.txt")
train_y<-read.table("train/Y_train.txt")
sub_test<-read.table("test/subject_test.txt")
test_y<-read.table("test/Y_test.txt")
## df is the data frame which combine the subject, activity and the measurement
## Every row in df corresponds to a activity
df<-data.frame(rbind(sub_train,sub_test),rbind(train_y,test_y),dfx_Keep)
names(df)<-c("Subject","Activity",names(dfx_Keep))
## Labeling the activity with info from the activity labels file
df_merge<-merge(df,labels,by.x="Activity",by.y="V1",all.x=TRUE,sort=FALSE)
df_merge<-data.frame(df_merge$V2,df_merge[,-c(1,ncol(df_merge))])
names(df_merge)<-c("Activity","Subject",names(dfx_Keep))
## Re-arrange the get data set df_merge as the [Subject, Activity, Measurment] form
df_merge<-df_merge[c(2,1,3:ncol(df_merge))]
## Group the data with subject
o<-order(df_merge$Subject,df_merge$Activity)
## Final Extracted data set
df_merge<-df_merge[o,]
## Output it to a file
write.table(df_merge, "mean_std.txt", sep="\t")
## Get the the average of each variable for each activity and each subject
## The col 3 is the first valid measurement
dfmean <- aggregate(df_merge[,3], by=list(df_merge$Activity, df_merge$Subject), FUN=mean)
## Merging all the following measurement to the required data frame
for(i in 4:ncol(df_merge)){
mean <- aggregate(df_merge[,i], by=list(Activity=df_merge$Activity, Subject=df_merge$Subject), FUN=mean)
dfmean<-cbind(dfmean,mean[,3])
}
## Re-organsize the data set as the final form
names(dfmean)<-c("Activity","Subject",names(dfx_Keep))
dfmean<-dfmean[c(2,1,3:ncol(dfmean))]
## Output it into a file
write.table(dfmean, "data_summary.txt", sep="\t")
setwd("C:/my home/coursera/Data Science Specialization/getting and clean data/course project/UCI HAR Dataset")
list.files()
## Setting work directory
setwd("C:/my home/coursera/Data Science Specialization/getting and clean data/course project/UCI HAR Dataset")
list.files()
## Inout train and test data
dfTrain_X<-read.table("./train/X_train.txt")
dfTest_X<-read.table("./test/X_test.txt")
df_X<-rbind(dfTrain_X,dfTest_X)
## Read features and assign to data frame
features<-read.table("features.txt",stringsAsFactors=FALSE)
colnames(df_X)<-features$V2
## Select the mean and standard deviztion cols and subsetting original data set
keep<-grepl("mean\\(\\)|std\\(\\)", names(df_X), ignore.case = TRUE)
dfx_Keep<-df_X[, keep]
## Read the activity labels
labels<-read.table("activity_labels.txt")
## Read the subject and training data set for each activity
sub_train<-read.table("./train/subject_train.txt")
train_y<-read.table("train/Y_train.txt")
sub_test<-read.table("test/subject_test.txt")
test_y<-read.table("test/Y_test.txt")
## df is the data frame which combine the subject, activity and the measurement
## Every row in df corresponds to a activity
df<-data.frame(rbind(sub_train,sub_test),rbind(train_y,test_y),dfx_Keep)
names(df)<-c("Subject","Activity",names(dfx_Keep))
## Labeling the activity with info from the activity labels file
df_merge<-merge(df,labels,by.x="Activity",by.y="V1",all.x=TRUE,sort=FALSE)
df_merge<-data.frame(df_merge$V2,df_merge[,-c(1,ncol(df_merge))])
names(df_merge)<-c("Activity","Subject",names(dfx_Keep))
## Re-arrange the get data set df_merge as the [Subject, Activity, Measurment] form
df_merge<-df_merge[c(2,1,3:ncol(df_merge))]
## Group the data with subject
o<-order(df_merge$Subject,df_merge$Activity)
## Final Extracted data set
df_merge<-df_merge[o,]
## Output it to a file
write.table(df_merge, "mean_std.txt", sep="\t")
df_merge
head(df_merge)
## Get the the average of each variable for each activity and each subject
## The col 3 is the first valid measurement
dfmean <- aggregate(df_merge[,3], by=list(df_merge$Activity, df_merge$Subject), FUN=mean)
## Merging all the following measurement to the required data frame
for(i in 4:ncol(df_merge)){
mean <- aggregate(df_merge[,i], by=list(Activity=df_merge$Activity, Subject=df_merge$Subject), FUN=mean)
dfmean<-cbind(dfmean,mean[,3])
}
## Re-organsize the data set as the final form
names(dfmean)<-c("Activity","Subject",names(dfx_Keep))
dfmean<-dfmean[c(2,1,3:ncol(dfmean))]
## Output it into a file
write.table(dfmean, "data_summary.txt", sep="\t")
dfmean
names(dfmean)
library("reshape")
dfm<-melt(df_merge, id=c("Subject","Activity"))
library("reshape2")
dfm<-melt(df_merge, id=c("Subject","Activity"))
dfm
head(dfm)
library("reshape2")
dfm<-melt(df_merge, id=c("Subject","Activity"))
dfmean <-  dcast(dfm, activity + subject ~ variable, mean)   #use dcast to create the final, tidy, data set.
dfmean <-  dcast(dfm, Subject + Activity ~ variable, mean)   #use dcast to create the final, tidy, data set.
library("reshape2")
dfm<-melt(df_merge, id=c("Subject","Activity"))
dfmean <-  dcast(dfm, Subject + Activity ~ variable, mean)   #use dcast to create the final, tidy, data set.
dfmean <-  dcast(dfm, Subject + Activity ~ variable)   #use dcast to create the final, tidy, data set.
dfmean
head(dfmean)
dfmean <-  dcast(dfm, Subject + Activity ~ variable, sum)   #use dcast to create the final, tidy, data set.
dfmean
dfmean <-  dcast(dfm, Subject + Activity ~ variable, mean)   #use dcast to create the final, tidy, data set.
library("reshape2")
dfm<-melt(df_merge, id=c("Subject","Activity"))
dfmean <-  dcast(dfm, Subject + Activity ~ variable, function(x)sum(x)/length(x))   #use dcast to create the final, tidy, data set.
dfmean
head(dfmean)
dfmean <-  dcast(dfm, Subject + Activity ~ variable, function(x)sum(x)/length(x))
## Output it into a file
write.table(dfmean, "data_summary.txt", sep="\t")
